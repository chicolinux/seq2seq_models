{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be51cd51-62fc-4cbe-ae77-25f301b1ba9a",
   "metadata": {},
   "source": [
    "# CSCI 4435/5435: Text Mining & Natural Language Processing\n",
    "# Assignment 6: Sequence to Sequence Models\n",
    "## Student: Miguel Guirao\n",
    "## Aggie ID: 800699208"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c3561-8b89-4e23-81eb-100802692a13",
   "metadata": {},
   "source": [
    "# Summary\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad508a5-f2ef-4d4f-adb5-f9c4a81e7a8f",
   "metadata": {},
   "source": [
    "## 0. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e7578a-9b7f-4822-a086-b2b30e73b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a006f90b-38bc-4fcc-8549-284ef9196a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2569decc-fc1e-4235-b817-18a93c02fbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cfca4406f8400aabccb0f38955434d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5560dc-f3fa-4a57-af5a-8f6e46ee134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset News Summary\n",
    "#summary_df = pd.read_csv('news_summary.csv')\n",
    "# Load a dataset from a local CSV file\n",
    "dataset = load_dataset(\"csv\", data_files=\"news_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b245b19c-bd1d-4a0a-b49f-baf6a27c7c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = dataset['train']\n",
    "type(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdb59db-f953-4065-b5a8-e729f4191949",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8452f3-e47b-4927-afa8-c0cc8ff51545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['headlines', 'text'],\n",
       "        num_rows: 78720\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['headlines', 'text'],\n",
       "        num_rows: 19681\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d38b9-44f1-4715-9da7-3a8622ef76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Language Translation English 2 French\n",
    "eng2french_df = pd.read_csv('eng_french.csv')\n",
    "eng2french_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d789c2e-c2aa-4822-b685-0e948d15b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2french_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4355c-ea7b-40b2-b6a4-ecaea6b95f1a",
   "metadata": {},
   "source": [
    "## 1. Task 1: Summarization\n",
    "### (40 pts) Finetuning a T5 model (you can use the Google t5-small pretrained model) for summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c343be7-ece5-42c6-bf58-bc81c8953bc5",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdb3eb77-519e-4ee0-b8e4-b31f122c05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function2(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"headlines\"], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e655264a-8457-4c29-bb80-9b11eb3203b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the source (input) texts\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",  # Ensure uniform length for batching\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the target (label) texts\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",  # Ensure labels are also padded\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    # Important for T5: replace pad token id with -100 so they are ignored in loss computation\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in l]\n",
    "        for l in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd396521-2847-4db5-9654-3f7aebe64156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f662c934a14f4aa3a0070bc95e2204f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/78720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1dc89daa5e4d8e9a5af8c9b1827392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ValueError: 128 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']\n",
    "tokenized_summary = summary.map(preprocess_function2, batched=True)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, label_pad_token_id=-100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fbd1891d-1bd7-4e0f-a418-ec2e790a4afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['headlines', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 78720\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['headlines', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19681\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae99fc02-cdd8-4446-9313-85463e7b6530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21603,\n",
       " 10,\n",
       " 22842,\n",
       " 4208,\n",
       " 961,\n",
       " 15,\n",
       " 120,\n",
       " 18,\n",
       " 9160,\n",
       " 11459,\n",
       " 17462,\n",
       " 14165,\n",
       " 23,\n",
       " 9,\n",
       " 65,\n",
       " 2162,\n",
       " 24,\n",
       " 8,\n",
       " 166,\n",
       " 999,\n",
       " 2250,\n",
       " 13,\n",
       " 165,\n",
       " 9279,\n",
       " 18,\n",
       " 17470,\n",
       " 7070,\n",
       " 443,\n",
       " 25766,\n",
       " 56,\n",
       " 281,\n",
       " 30,\n",
       " 1048,\n",
       " 17078,\n",
       " 16,\n",
       " 4144,\n",
       " 37,\n",
       " 25766,\n",
       " 19,\n",
       " 1644,\n",
       " 12,\n",
       " 1111,\n",
       " 44,\n",
       " 3,\n",
       " 9,\n",
       " 1634,\n",
       " 13,\n",
       " 300,\n",
       " 11321,\n",
       " 3,\n",
       " 157,\n",
       " 7656,\n",
       " 16,\n",
       " 20527,\n",
       " 607,\n",
       " 28,\n",
       " 3,\n",
       " 9,\n",
       " 620,\n",
       " 13,\n",
       " 300,\n",
       " 431,\n",
       " 4906,\n",
       " 2280,\n",
       " 5,\n",
       " 37,\n",
       " 5449,\n",
       " 56,\n",
       " 3971,\n",
       " 12,\n",
       " 3,\n",
       " 9,\n",
       " 2411,\n",
       " 491,\n",
       " 6592,\n",
       " 13,\n",
       " 220,\n",
       " 2280,\n",
       " 5,\n",
       " 1]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60b2cdf5-9498-490f-8a3c-36da5b07cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20a9164c-b265-4b9e-93b8-e3e72350d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d5ab1a3-13dd-47c1-89bc-961f4e6618bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e52e4def-a9dc-4c94-8797-8f4ff75b440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4921' max='19680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4921/19680 19:01 < 57:04, 4.31 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1231' max='1231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1231/1231 05:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOverflowError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      1\u001b[39m training_args = Seq2SeqTrainingArguments(\n\u001b[32m      2\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     push_to_hub=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     16\u001b[39m     model=model,\n\u001b[32m     17\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/trainer.py:2236\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2233\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2234\u001b[39m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[32m   2235\u001b[39m     hf_hub_utils.disable_progress_bars()\n\u001b[32m-> \u001b[39m\u001b[32m2236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2237\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2243\u001b[39m     hf_hub_utils.enable_progress_bars()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/trainer.py:2661\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2658\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2660\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2661\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2667\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/trainer.py:3096\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3094\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3096\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3097\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3099\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/trainer.py:3045\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3044\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3048\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:197\u001b[39m, in \u001b[36mSeq2SeqTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather\n\u001b[32m    196\u001b[39m \u001b[38;5;28mself\u001b[39m._gen_kwargs = gen_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/trainer.py:4154\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4151\u001b[39m start_time = time.time()\n\u001b[32m   4153\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4154\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4155\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4164\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/trainer.py:4443\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4441\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33mlosses\u001b[39m\u001b[33m\"\u001b[39m] = all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4442\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m] = all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4443\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4446\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4447\u001b[39m     metrics = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(eval_pred)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_metrics\u001b[39m(eval_pred):\n\u001b[32m      2\u001b[39m     predictions, labels = eval_pred\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     decoded_preds = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     labels = np.where(labels != -\u001b[32m100\u001b[39m, labels, tokenizer.pad_token_id)\n\u001b[32m      5\u001b[39m     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3831\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_decode\u001b[39m\u001b[34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3808\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3811\u001b[39m     **kwargs,\n\u001b[32m   3812\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3813\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3814\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3815\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3828\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3830\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m3831\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3835\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3836\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[32m   3838\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3870\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3867\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3868\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3870\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3874\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3875\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:668\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    667\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    671\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    672\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    674\u001b[39m )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mOverflowError\u001b[39m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_summary[\"train\"],\n",
    "    eval_dataset=tokenized_summary[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bda074-6608-4467-9f5f-8dbf07bc02c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
